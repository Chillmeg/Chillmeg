{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chillmeg/Chillmeg/blob/main/6_7960_Fall_2025_hw2_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Architecture and inductive bias"
      ],
      "metadata": {
        "id": "T_F-vTYIALgk"
      },
      "id": "T_F-vTYIALgk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** For problem 1 in this notebook, include your answers in the write pdf submission.\n",
        "\n",
        "We will use the CIFAR-100 in this question, which has **100** classes. Unlike homework 1, we will instead use CNN models to investigate some interesting aspects of training deep models. Moreover, we will use standard PyTorch `nn.Module`s and `optim.Optimizer`s.\n",
        "\n",
        "<h2>CIFAR-100</h2>\n",
        "\n",
        "CIFAR-100  is a image classification dataset.\n",
        "+ Each data sample is an RGB $32\\times32$ real image. A raw loaded image $\\in \\mathbb{R}^{3 \\times 32 \\times 32}$.\n",
        "+ Each image is associated with a label $\\in \\{0,1,2,\\dots, 99\\}$.\n",
        "\n",
        "\n",
        "Our goal is to train a neural network classifier that takes such $3\\times32\\times32$ images and predict a label $\\in \\{0, 1, 2, \\dots, 99\\}$."
      ],
      "metadata": {
        "id": "vm1bwIXFAvF8"
      },
      "id": "vm1bwIXFAvF8"
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "yO6sJlx6Auwk"
      },
      "id": "yO6sJlx6Auwk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should run on GPU-enabled colab server (should be default for this notebook)."
      ],
      "metadata": {
        "id": "hZeZjIJZBB2y"
      },
      "id": "hZeZjIJZBB2y"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from typing import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dataclasses\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "assert torch.cuda.is_available(), \"Should use GPU-enabled colab\"\n",
        "\n",
        "device = torch.device('cuda:0')  # we will train with CUDA!"
      ],
      "metadata": {
        "id": "i--6u78-BBnv"
      },
      "id": "i--6u78-BBnv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset\n",
        "Let's reuse the homework 1 augmented dataset code, but now modified to work on **CIFAR-100**. Code is hidden in the cell below, but make sure to still run it."
      ],
      "metadata": {
        "id": "WIsVLBq80Fa4"
      },
      "id": "WIsVLBq80Fa4"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_datasets(train_transforms=(), val_transforms=()):\n",
        "    r\"\"\"\n",
        "    Returns the CIFAR-100 training and validation datasets with corresponding\n",
        "    transforms.\n",
        "\n",
        "    `*_transforms` represent optional transformations, e.g., conversion to\n",
        "    PyTorch tensors, preprocessing, etc.\n",
        "    \"\"\"\n",
        "    train_set = torchvision.datasets.CIFAR100(\n",
        "        './data', train=True, download=True,\n",
        "        transform=torchvision.transforms.Compose(train_transforms))\n",
        "    val_set = torchvision.datasets.CIFAR100(\n",
        "        './data', train=False, download=True,\n",
        "        transform=torchvision.transforms.Compose(val_transforms))\n",
        "    return train_set, val_set\n",
        "\n",
        "\n",
        "cifar100_mean = torch.as_tensor([0.5071, 0.4865, 0.4409])\n",
        "cifar100_std = torch.as_tensor([0.2673, 0.2564, 0.2762])\n",
        "\n",
        "train_transforms = [\n",
        "    torchvision.transforms.RandomCrop(32, padding=3, padding_mode='reflect'),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=cifar100_mean, std=cifar100_std),\n",
        "]\n",
        "\n",
        "val_transforms = [\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=cifar100_mean, std=cifar100_std),\n",
        "]\n",
        "\n",
        "train_set, val_set = get_datasets(train_transforms, val_transforms)\n",
        "\n",
        "print(f\"Training set size: {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "\n",
        "class_names = train_set.classes\n",
        "print(f'CIFAR-100 classes: {class_names}')\n",
        "\n",
        "def visualize_tensor_data(data: torch.Tensor, label: int):\n",
        "    # Data is a tensor of shape [C, W, H]  (C is the channel dimension, 3 for RGB)\n",
        "    # Put channel at last\n",
        "    data = data.permute(1, 2, 0)\n",
        "    # Un-normalize\n",
        "    data = data * cifar100_std + cifar100_mean\n",
        "\n",
        "    plt.imshow(data)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Label = {class_names[label]}')\n",
        "\n",
        "data, label = train_set[13]\n",
        "visualize_tensor_data(data, label)\n"
      ],
      "metadata": {
        "id": "fZhZ92x10LnO"
      },
      "id": "fZhZ92x10LnO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Definition\n",
        "\n",
        "**Part 1**: Implement the following function that creates the specified CNN networks. Read through the docstring and make sure that your implementation behaves as required. Afterwards, print the outputs of\n",
        "+ `make_cnn(100, 'tanh', num_conv_layers=5, num_fc_layers=4)`\n",
        "+ `make_cnn(3, 'none', num_conv_layers=3, num_fc_layers=2)`\n",
        "\n",
        "Add both your implementation and outputs to the writeup.\n",
        "\n",
        "---\n",
        "\n",
        "Links you may find useful:\n",
        "+ `none` activation: https://pytorch.org/docs/stable/generated/torch.nn.Identity.html?highlight=identity#torch.nn.Identity\n",
        "+ Chaining layers: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential\n",
        "+ Reshaping image-like tensors to vectors: https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html?highlight=flatten#torch.nn.Flatten\n",
        "\n",
        "(You are not required to use these classes)"
      ],
      "metadata": {
        "id": "GqHWzTlQ0OQ0"
      },
      "id": "GqHWzTlQ0OQ0"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mlp(num_inputs: int, num_outputs: int, activation: str, hidden_sizes: int = [128]) -> nn.Module:\n",
        "\n",
        "    if activation == 'relu':\n",
        "        act_cls = nn.ReLU\n",
        "    elif activation == 'tanh':\n",
        "        act_cls = nn.Tanh\n",
        "    elif activation == 'none':\n",
        "        act_cls = nn.Identity\n",
        "    else:\n",
        "        raise ValueError(f'Unexpected activation={repr(activation)}')\n",
        "\n",
        "    net = [nn.Flatten(),\n",
        "           nn.Linear(num_inputs, hidden_sizes[0]),\n",
        "           act_cls()]\n",
        "\n",
        "    for i in range(1, len(hidden_sizes)):\n",
        "      net.extend([nn.Linear(hidden_sizes[i-1], hidden_sizes[i]),\n",
        "                  act_cls()])\n",
        "\n",
        "    net.extend([nn.Linear(hidden_sizes[-1], num_outputs)])\n",
        "\n",
        "    return nn.Sequential(*net)"
      ],
      "metadata": {
        "id": "X_zhw8jw0fkk"
      },
      "id": "X_zhw8jw0fkk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cnn(num_outputs: int, activation: str, num_conv_layers: int = 4, num_fc_layers: int = 2) -> nn.Module:\n",
        "    r'''\n",
        "    Returns a PyTorch module representing a CNN network that takes in image-like input with shape [3, 32, 32].\n",
        "\n",
        "    Args:\n",
        "        num_outputs (int): size of the final output layer. E.g., if the network is a classifier, this is usually #classes.\n",
        "        activation (str): activation functions between conv/linear layers. Supported choices are ['relu', 'tanh', 'none'] (always strings).\n",
        "            For 'none', no activation function is applied, and the previous conv/linear output is directly fed into the next conv/linear.\n",
        "        num_conv_layers (int): number of conv layers in CNN. This should be >= 3 and <= 5.\n",
        "            First conv layer should always use 5x5 kernels, mapping 3-channel data to 12-channel data, with **reflect** padding=2 and no striding.\n",
        "            Subsequent conv layers should always use 3x3 kernels, mapping to 64-channel data, with zeros (default) padding=1 and strides=2.\n",
        "        num_fc_layers (int): number of fc/linear layers after the convolutional part. This should be >= 2.\n",
        "            For all but the last fc layer, it should output 128-dimensional vectors.\n",
        "\n",
        "    Returns:\n",
        "        (nn.Module) The CNN network of desired tructure.\n",
        "\n",
        "    If an unexpected input is given, this will raise a ValueError. (You can assume that arguments are of correct types.)\n",
        "    '''\n",
        "\n",
        "    # FIXME\n",
        "    return NotImplemented"
      ],
      "metadata": {
        "id": "CWatWTl20g5l"
      },
      "id": "CWatWTl20g5l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('num_outputs=100, tanh, hidden_sizes=[128, 120]')\n",
        "print(make_mlp(10, 100, 'tanh', hidden_sizes=[128, 120]))\n",
        "print()\n",
        "print('num_outputs=100, tanh, num_conv_layers=5, num_fc_layers=4:')\n",
        "print(make_cnn(100, 'tanh', num_conv_layers=5, num_fc_layers=4))\n",
        "print()\n",
        "print('num_outputs=3, no activation, num_conv_layers=3, num_fc_layers=2:')\n",
        "print(make_cnn(3, 'none', num_conv_layers=3, num_fc_layers=2))"
      ],
      "metadata": {
        "id": "liMpbwS-0kPE"
      },
      "id": "liMpbwS-0kPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "\n",
        "**Part 2**: Implement the following training and evaluate functions according to docstring. Complete the `FIXME`s in `train_epoch`, `evaluate`, and `train`. Attach your code to PDF.\n",
        "\n",
        "---\n",
        "\n",
        "Links you may find useful:\n",
        "+ Classification loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy\n",
        "+ Learning rate adjustment: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "(You are not required to use these classes)"
      ],
      "metadata": {
        "id": "8_Kw85oW0mi-"
      },
      "id": "8_Kw85oW0mi-"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(epoch: int, model: nn.Module, train_loader: torch.utils.data.DataLoader, optim: torch.optim.Optimizer):\n",
        "    r\"\"\"\n",
        "    Trains `model` on `train_loader` for cross entropy loss for one epoch.\n",
        "\n",
        "    Args:\n",
        "        epoch (int): the current epoch number (i.e., number of epochs done before this one).\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "        train_loader (DataLoader): a DataLoader that yields (batched_images, batched_target_class_indices) when iterated.\n",
        "        optim (Optimizer): optimizer object that is created with `model` parameters and should be used for updating `model` in this function.\n",
        "\n",
        "    Returns:\n",
        "        (List[float]) the losses computed at each iteration as a list of *Python* numbers.\n",
        "\n",
        "    NOTE:\n",
        "      + Remember to clear previously computed gradient at beginning of each iteration.\n",
        "      + Convert loss values to python floats (e.g., via `.item()`) before adding to `loss_values`.\n",
        "    \"\"\"\n",
        "    loss_values: List[float] = []\n",
        "\n",
        "    for data, target in tqdm(train_loader, desc=f'Training @ epoch {epoch}'):\n",
        "        # FIXME\n",
        "        pass\n",
        "\n",
        "    return loss_values\n",
        "\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class EvaluateResult:\n",
        "    r\"\"\"\n",
        "    A collection containing everything we need to know about the evaluate results.\n",
        "\n",
        "    See `evaluate` docstring for meanings of the members of this class\n",
        "    \"\"\"\n",
        "    acc: float  # overall accuracy\n",
        "    correct_predictions: torch.Tensor  # size |dataset|\n",
        "    confidence: torch.Tensor  # size |dataset|\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: torch.utils.data.DataLoader) -> EvaluateResult:\n",
        "    r\"\"\"\n",
        "    Evaluate a classifier `model` on dataset contained in `loader`.\n",
        "\n",
        "    For each input, the predicted label is taken as one with highest probability in the distribution given by `model`.\n",
        "\n",
        "    In addition to compute overall accuracies, we also output\n",
        "        (1) a boolean Tensor with size |dataset|, showing whether each sample is correctly classified.\n",
        "        (2) a float32 Tensor with size |dataset|, showing the `model`'s assigned probability for its prediction, called *confidence*.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "        loader (DataLoader): a DataLoader that yields (batched_images, batched_target_class_indices) when iterated.\n",
        "\n",
        "    Returns:\n",
        "        (EvaluateResult) Containing overall accuracy, whether each sample is correctly classified, and confidence.\n",
        "            The tensors should be on **CPU**.\n",
        "    \"\"\"\n",
        "    # FIXME\n",
        "    pass\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class TrainResult:\n",
        "    r\"\"\"\n",
        "    A collection containing everything we need to know about the training results\n",
        "    \"\"\"\n",
        "    num_epochs: int\n",
        "\n",
        "    # Training loss (saved at each iteration in `train_epoch`)\n",
        "    train_losses: List[float]\n",
        "\n",
        "    # The epochs where we perform evaluation\n",
        "    eval_epochs: List[int]\n",
        "\n",
        "    # Training accuracies, computed at each epoch in `eval_epochs`\n",
        "    train_accs: List[float]\n",
        "\n",
        "    # Validation accuracies, computed at each epoch in `eval_epochs`\n",
        "    val_accs: List[float]\n",
        "\n",
        "    # The last validation evaluation full result\n",
        "    final_val_eval_result: EvaluateResult = None\n",
        "\n",
        "\n",
        "def train(model: nn.Module, train_set, val_set, *, num_epochs=60, lr=0.003, train_epoch_fn=train_epoch, **kwargs):\n",
        "    r\"\"\"\n",
        "    Train `model` on `train_set` for `num_epochs` epochs using **Adam** optimizer and `lr` learning rate\n",
        "    following a decay schedule by a factor of `0.3` at epoch 5`.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): our network (created using `make_cnn`).\n",
        "        train_set (Dataset): CIFAR-100 training dataset.\n",
        "        val_set (Dataset): CIFAR-100 validation dataset. Evaluated every *5* epochs and at the end of training.\n",
        "        num_epochs (int): number of total training epochs.\n",
        "        lr (float): initial learning rate.\n",
        "        train_epoch_fn (Callable): function that trains the model for a single epoch. This is `train_epoch`\n",
        "            usually, but we will use different choices in later questions.\n",
        "\n",
        "    Returns:\n",
        "        (TrainResult)\n",
        "    \"\"\"\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=512, shuffle=True)  # Random order for training (\"[S]tochastic\" in SGD)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=1024, shuffle=False)\n",
        "\n",
        "    # Our classifier\n",
        "    print('Model =', model)\n",
        "\n",
        "    # Create optimizer and lr scheduler\n",
        "    # FIXME\n",
        "    optim = '?????'\n",
        "    scheduler = '?????'\n",
        "\n",
        "    result: TrainResult = TrainResult(num_epochs, train_losses=[], eval_epochs=[], train_accs=[], val_accs=[])\n",
        "    last_eval_epoch = -float('inf')\n",
        "\n",
        "    # Iterate through the entire training dataset `num_epochs` times\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch - last_eval_epoch >= 5:\n",
        "            result.eval_epochs.append(epoch)\n",
        "            result.train_accs.append(evaluate(model, train_loader).acc)\n",
        "            result.val_accs.append(evaluate(model, val_loader).acc)\n",
        "            print(f\"Epoch = {epoch:> 2d}    Train acc = {result.train_accs[-1]:.2%}    Val acc = {result.val_accs[-1]:.2%}\")\n",
        "            last_eval_epoch = epoch\n",
        "        # Train over the entire `train_set` with given `train_epoch_fn` function (i.e., one epoch)\n",
        "        result.train_losses.extend(train_epoch_fn(epoch, model, train_loader, optim, **kwargs))\n",
        "        # Evaluate with our `evaluate` function\n",
        "        print(f\"Epoch = {epoch:> 2d}    Train loss = {result.train_losses[-1]:.4f}\")\n",
        "\n",
        "        # Adjust learning rate if needed\n",
        "        scheduler.step()\n",
        "\n",
        "    result.eval_epochs.append(num_epochs)\n",
        "    result.train_accs.append(evaluate(model, train_loader).acc)\n",
        "    result.final_val_eval_result = evaluate(model, val_loader)\n",
        "    result.val_accs.append(result.final_val_eval_result.acc)\n",
        "    print(f\"Epoch = {num_epochs:> 2d}    Train acc = {result.train_accs[-1]:.2%}    Val acc = {result.val_accs[-1]:.2%}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "n4ecHtRa0ouw"
      },
      "id": "n4ecHtRa0ouw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide the following learning curve plotting function (similar to homework 1)."
      ],
      "metadata": {
        "id": "xfCyggIM0qag"
      },
      "id": "xfCyggIM0qag"
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_curve(result: TrainResult, *, title: str = 'Learning Curve'):\n",
        "    r\"\"\"\n",
        "    Plot the training loss, training accuracy, and validation accuracy versus\n",
        "    epochs taken.\n",
        "    \"\"\"\n",
        "    fig, ax_loss = plt.subplots(figsize=(8, 5))\n",
        "    ax_loss.set_title(title, fontsize=16)\n",
        "    ax_loss.set_xlabel('Epoch', fontsize=12)\n",
        "\n",
        "    l_trloss = ax_loss.plot(\n",
        "        torch.arange(len(result.train_losses)) / len(result.train_losses) * result.num_epochs,\n",
        "        result.train_losses,\n",
        "        label='Train loss',\n",
        "        color='C0',\n",
        "    )\n",
        "    ax_loss.set_ylim(0, 4.8)\n",
        "    ax_loss.set_ylabel('Train loss', color='C0', fontsize=12)\n",
        "    ax_loss.tick_params(axis='y', labelcolor='C0')\n",
        "\n",
        "    ax_acc = ax_loss.twinx()\n",
        "    l_tracc = ax_acc.plot(result.eval_epochs, result.train_accs, label='Train acc', color='C1', linestyle='--')\n",
        "    if len(result.val_accs):\n",
        "        l_valacc = ax_acc.plot(result.eval_epochs, result.val_accs, label='Val acc', color='C1')\n",
        "    else:\n",
        "        l_valacc = []\n",
        "    ax_acc.set_ylim(0, 1)\n",
        "    ax_acc.set_ylabel('Accuracies', color='C1', fontsize=12)\n",
        "    ax_acc.tick_params(axis='y', labelcolor='C1')\n",
        "\n",
        "    lines = l_trloss + l_tracc + l_valacc\n",
        "    ax_loss.legend(lines, [l.get_label() for l in lines], loc='upper left', fontsize=13)"
      ],
      "metadata": {
        "id": "wVyKChcp0tvu"
      },
      "id": "wVyKChcp0tvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MLP 128 width, 3 depth on the GPU, train it, and plot learning curves."
      ],
      "metadata": {
        "id": "o5vkqHti7m25"
      },
      "id": "o5vkqHti7m25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MLP 128 width, 7 depth on the GPU, train it, and plot learning curves."
      ],
      "metadata": {
        "id": "uAltdfaM7oDH"
      },
      "id": "uAltdfaM7oDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN 4 convolutional layers, 3 MLP layers on the GPU, train it, and plot learning curves."
      ],
      "metadata": {
        "id": "jeW_XSZj7o7Y"
      },
      "id": "jeW_XSZj7o7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eb743ee"
      },
      "source": [
        "# Problem 3: Implement a Transformer"
      ],
      "id": "6eb743ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47cd59de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple, Union, Optional, List\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "id": "47cd59de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baab5229"
      },
      "outputs": [],
      "source": [
        "# a utility for calculating running average\n",
        "class AverageMeter():\n",
        "    def __init__(self):\n",
        "        self.num = 0\n",
        "        self.tot = 0\n",
        "\n",
        "    def update(self, val: float, sz: float):\n",
        "        self.num += val*sz\n",
        "        self.tot += sz\n",
        "\n",
        "    def calculate(self) -> float:\n",
        "        return self.num/self.tot"
      ],
      "id": "baab5229"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6cffe0a"
      },
      "source": [
        "## Part 3.A"
      ],
      "id": "e6cffe0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29fc4254"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int):\n",
        "        # dim: the dimension of the input\n",
        "        # n_hidden: the dimension of the keys, queries, and values\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_K = nn.Linear(dim, n_hidden) # W_K weight matrix\n",
        "        self.W_Q = nn.Linear(dim, n_hidden) # W_Q weight matrix\n",
        "        self.W_V = nn.Linear(dim, n_hidden) # W_V weight matrix\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      the output of performing self-attention on x. shape: (Batch x Num_tokens x n_hidden)\n",
        "        # alpha            the attention weights (after softmax). shape: (B x T x T)\n",
        "        #\n",
        "\n",
        "        out, alpha = None, None\n",
        "        # TODO: Compute self attention on x.\n",
        "        #       (1) First project x to the query Q, key K, value V.\n",
        "        #       (2) Then compute the attention weights alpha as:\n",
        "        #                  alpha = softmax(QK^T/sqrt(n_hidden))\n",
        "        #           Make sure to take into account attn_mask such that token i does not attend on token\n",
        "        #           j if attn_mask[b, i, j] == 0. (Hint, in such a case, what value should you set the weight\n",
        "        #           to before the softmax so that after the softmax the value is 0?)\n",
        "        #       (3) The output is a linear combination of the values (weighted by the alphas):\n",
        "        #                  out = alpha V\n",
        "        #       (4) return the output and the alpha after the softmax\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer  END ========\n",
        "\n",
        "        return attn_output, alpha\n"
      ],
      "id": "29fc4254"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d9cb9ad"
      },
      "source": [
        "## Part 3.B"
      ],
      "id": "7d9cb9ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddd56734"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int, num_heads: int):\n",
        "        # dim: the dimension of the input\n",
        "        # n_hidden: the hidden dimenstion for the attention layer\n",
        "        # num_heads: the number of attention heads\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: set up your parameters for multi-head attention. You should initialize\n",
        "        #       num_heads attention heads (see nn.ModuleList) as well as a linear layer\n",
        "        #       that projects the concatenated outputs of each head into dim\n",
        "        #       (what size should this linear layer be?)\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer  END ========\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      the output of performing multi-headed self-attention on x.\n",
        "        #                  shape: (B x T x dim)\n",
        "        # attn_alphas      the attention weights of each of the attention heads.\n",
        "        #                  shape: (B x Num_heads x T x T)\n",
        "\n",
        "        attn_output, attn_alphas = None, None\n",
        "\n",
        "        # TODO: Compute multi-headed attention. Loop through each of your attention heads\n",
        "        #       and collect the outputs. Concatenate them together along the hidden dimension,\n",
        "        #       and then project them back into the output dimension (dim). Return both\n",
        "        #       the final attention outputs as well as the alphas from each head.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "        return attn_output, attn_alphas"
      ],
      "id": "ddd56734"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8012740"
      },
      "source": [
        "## Part 3.C"
      ],
      "id": "a8012740"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8531bc02"
      },
      "outputs": [],
      "source": [
        "# these are already implemented for you!\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int):\n",
        "        # dim       the dimension of the input\n",
        "        # n_hidden  the width of the linear layer\n",
        "\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, n_hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(n_hidden, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "        # x         the input. shape: (B x T x dim)\n",
        "\n",
        "        # Outputs:\n",
        "        # out       the output of the feed-forward network: (B x T x dim)\n",
        "        return self.net(x)\n",
        "\n",
        "class AttentionResidual(nn.Module):\n",
        "    def __init__(self, dim: int, attn_dim: int, mlp_dim: int, num_heads: int):\n",
        "        # dim       the dimension of the input\n",
        "        # attn_dim  the hidden dimension of the attention layer\n",
        "        # mlp_dim   the hidden layer of the FFN\n",
        "        # num_heads the number of heads in the attention layer\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadedAttention(dim, attn_dim, num_heads)\n",
        "        self.ffn = FFN(dim, mlp_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
        "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
        "        #                  otherwise. shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      shape: (B x T x dim)\n",
        "        # attn_alphas      the attention weights of each of the attention heads.\n",
        "        #                  shape: (B x Num_heads x T x T)\n",
        "\n",
        "        attn_out, alphas = self.attn(x=x, attn_mask=attn_mask)\n",
        "        x = attn_out + x\n",
        "        x = self.ffn(x) + x\n",
        "        return x, alphas"
      ],
      "id": "8531bc02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02c21aea"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim: int, attn_dim: int, mlp_dim: int, num_heads: int, num_layers: int):\n",
        "        # dim       the dimension of the input\n",
        "        # attn_dim  the hidden dimension of the attention layer\n",
        "        # mlp_dim   the hidden layer of the FFN\n",
        "        # num_heads the number of heads in the attention layer\n",
        "        # num_layers the number of attention layers.\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: set up the parameters for the transformer!\n",
        "        #       You should set up num_layers of AttentionResiduals\n",
        "        #       nn.ModuleList will be helpful here.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor, return_attn=False)-> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # x                the inputs. shape: (B x T x dim)\n",
        "        # attn_mask        an attention mask. Pass this to each of the AttentionResidual layers!\n",
        "        #                  shape: (B x T x T)\n",
        "        #\n",
        "        # Outputs:\n",
        "        # attn_output      shape: (B x T x dim)\n",
        "        # attn_alphas      If return_attn is False, return None. Otherwise return the attention weights\n",
        "        #                  of each of each of the attention heads for each of the layers.\n",
        "        #                  shape: (B x Num_layers x Num_heads x T x T)\n",
        "\n",
        "        output, collected_attns = None, None\n",
        "\n",
        "        # TODO: Implement the transformer forward pass! Pass the input successively through each of the\n",
        "        # AttentionResidual layers. If return_attn is True, collect the alphas along the way.\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n"
      ],
      "id": "02c21aea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "643d379b"
      },
      "source": [
        "Test your transformer implementation here"
      ],
      "id": "643d379b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aaa156e"
      },
      "outputs": [],
      "source": [
        "def perform_transformer_test_cases():\n",
        "    num_tokens = 100\n",
        "    batch_size = 10\n",
        "    dim = 64\n",
        "    num_layers = 4\n",
        "    num_heads = 2\n",
        "    dummy_model = Transformer(dim=dim, attn_dim=32, mlp_dim=dim, num_heads=num_heads, num_layers=num_layers).cuda()\n",
        "\n",
        "    inp = torch.randn(batch_size, num_tokens, dim).cuda()\n",
        "\n",
        "    # test case 1 regular forward pass\n",
        "    print(\"Test Case 1\")\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=None)\n",
        "        assert alpha is None\n",
        "        assert output.shape == (batch_size, num_tokens, dim), f\"wrong output shape {output.shape}\"\n",
        "\n",
        "    # test case 2 collect attentions\n",
        "    print(\"Test Case 2\")\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=None, return_attn=True)\n",
        "        assert output.shape == (batch_size, num_tokens, dim), f\"wrong output shape {output.shape}\"\n",
        "        assert alpha.shape == (batch_size, num_layers, num_heads, num_tokens, num_tokens), f\"wrong alpha shape {alpha.shape}\"\n",
        "\n",
        "    print(\"Test Case 3\")\n",
        "    # test case 3 with attention mask\n",
        "    attn_mask = torch.zeros(batch_size, num_tokens, num_tokens).cuda()\n",
        "    attn_mask[:, torch.arange(num_tokens), torch.arange(num_tokens)] = 1\n",
        "    attn_mask[:, torch.arange(num_tokens)[1:], torch.arange(num_tokens)[:-1]] = 1\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=attn_mask, return_attn=True)\n",
        "        print(\"Attention mask pattern\", attn_mask[0])\n",
        "        print(\"Alpha pattern\", alpha[0, 0, 0])\n",
        "        assert torch.all(alpha.permute(1, 2, 0, 3, 4)[:, :, attn_mask == 0] == 0).item()\n",
        "\n",
        "    print(\"Test Case 4\")\n",
        "    # test case 4 creates a causal mask where each token can only attend to previous tokens and itself\n",
        "    causal_mask = torch.tril(torch.ones(num_tokens, num_tokens)).unsqueeze(0).repeat(batch_size, 1, 1)  # Shape: (B, T, T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output, alpha = dummy_model(inp, attn_mask=causal_mask, return_attn=True)\n",
        "        # Verify the causal mask\n",
        "        for b in range(batch_size):\n",
        "            for l in range(num_layers):\n",
        "                for h in range(num_heads):\n",
        "                    attn_weights = alpha[b, l, h]  # Shape: (T, T)\n",
        "                    # Positions where j > i should have zero attention weights\n",
        "                    # We can create a boolean mask for j > i\n",
        "                    future_mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1).bool()  # Shape: (T, T)\n",
        "                    # Extract attention weights for future positions\n",
        "                    future_attn = attn_weights[future_mask]\n",
        "                    # Assert that these weights are close to zero\n",
        "                    assert torch.all(future_attn < 1e-6), f\"Causal mask violated in batch {b}, layer {l}, head {h}\"\n",
        "\n",
        "perform_transformer_test_cases()"
      ],
      "id": "9aaa156e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afc7e9b"
      },
      "source": [
        "## Problem 4: Vision Transformer"
      ],
      "id": "5afc7e9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b725d5"
      },
      "source": [
        "## Part 4.A"
      ],
      "id": "67b725d5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dc4ea76"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size: int, patch_size: int, nin: int, nout: int):\n",
        "        # img_size       the width and height of the image. you can assume that\n",
        "        #                the images will be square\n",
        "        # patch_size     the width of each square patch. You can assume that\n",
        "        #                img_size is divisible by patch_size\n",
        "        # nin            the number of input channels\n",
        "        # nout           the number of output channels\n",
        "\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.num_patches = (img_size // patch_size)**2\n",
        "\n",
        "        # TODO Set up parameters for the Patch Embedding\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x        the input image. shape: (B, nin, Height, Width)\n",
        "        #\n",
        "        # Output\n",
        "        # out      the patch embeddings for the input. shape: (B, num_patches, nout)\n",
        "\n",
        "\n",
        "        # TODO: Implement the patch embedding. You want to split up the image into\n",
        "        # square patches of the given patch size. Then each patch_size x patch_size\n",
        "        # square should be linearly projected into an embedding of size nout.\n",
        "        #\n",
        "        # Hint: Take a look at nn.Conv2d. How can this be used to perform the\n",
        "        #       patch embedding?\n",
        "        out = None\n",
        "\n",
        "        # ======= Answer START ========\n",
        "\n",
        "        # ======= Answer END ========\n",
        "\n",
        "        return out"
      ],
      "id": "0dc4ea76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57d32978"
      },
      "source": [
        "## Part 4.B"
      ],
      "id": "57d32978"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df3d8a7a"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, n_channels: int, nout: int, img_size: int, patch_size: int, dim: int, attn_dim: int,\n",
        "                 mlp_dim: int, num_heads: int, num_layers: int):\n",
        "        # n_channels       number of input image channels\n",
        "        # nout             desired output dimension\n",
        "        # img_size         width of the square image\n",
        "        # patch_size       width of the square patch\n",
        "        # dim              embedding dimension\n",
        "        # attn_dim         the hidden dimension of the attention layer\n",
        "        # mlp_dim          the hidden layer dimension of the FFN\n",
        "        # num_heads        the number of heads in the attention layer\n",
        "        # num_layers       the number of attention layers.\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, nin=n_channels, nout=dim)\n",
        "        self.pos_E = nn.Embedding((img_size//patch_size)**2, dim) # positional embedding matrix\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) # learned class embedding\n",
        "        self.transformer = Transformer(\n",
        "            dim=dim, attn_dim=attn_dim, mlp_dim=mlp_dim, num_heads=num_heads, num_layers=num_layers)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, nout)\n",
        "        )\n",
        "\n",
        "    def forward(self, img: torch.Tensor, return_attn=False) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        # img          the input image. shape: (B, nin, img_size, img_size)\n",
        "        # return_attn  whether to return the attention alphas\n",
        "        #\n",
        "        # Outputs\n",
        "        # out          the output of the vision transformer. shape: (B, nout)\n",
        "        # alphas       the attention weights for all heads and layers. None if return_attn is False, otherwise\n",
        "        #              shape: (B, num_layers, num_heads, num_patches + 1, num_patches + 1)\n",
        "\n",
        "        # generate embeddings\n",
        "        embs = self.patch_embed(img) # patch embedding\n",
        "        B, T, _ = embs.shape\n",
        "        pos_ids = torch.arange(T).expand(B, -1).to(embs.device)\n",
        "        embs += self.pos_E(pos_ids) # positional embedding\n",
        "\n",
        "        cls_token = self.cls_token.expand(len(embs), -1, -1)\n",
        "        x = torch.cat([cls_token, embs], dim=1)\n",
        "\n",
        "        x, alphas = self.transformer(x, attn_mask=None, return_attn=return_attn)\n",
        "        out = self.head(x)[:, 0]\n",
        "        return out, alphas\n",
        "\n"
      ],
      "id": "df3d8a7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bcc81df"
      },
      "source": [
        "## Part 4.C"
      ],
      "id": "8bcc81df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1107044"
      },
      "outputs": [],
      "source": [
        "# set up the dataset and dataloader\n",
        "\n",
        "MEAN = [0.4914, 0.4822, 0.4465]\n",
        "STD = [0.2470, 0.2435, 0.2616]\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "inv_transform = transforms.Compose([\n",
        "    transforms.Normalize(\n",
        "        mean = [ 0., 0., 0. ],\n",
        "        std = 1/np.array(STD)),\n",
        "    transforms.Normalize(\n",
        "        mean = -np.array(MEAN),\n",
        "        std = [ 1., 1., 1. ]),\n",
        "    transforms.ToPILImage(),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(train=True, root='data', transform=img_transform, download=True)\n",
        "val_dataset = torchvision.datasets.CIFAR10(train=False, root='data', transform=img_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=10)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=10)"
      ],
      "id": "e1107044"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2540d4b3"
      },
      "outputs": [],
      "source": [
        "# set up the model and optimizer\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "model = VisionTransformer(n_channels=3, nout=10, img_size=32, patch_size=4,\n",
        "                          dim=128, attn_dim=64, mlp_dim=128, num_heads=3, num_layers=6).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n"
      ],
      "id": "2540d4b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147e1bc8"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "def evaluate_cifar_model(model, criterion, val_loader):\n",
        "    is_train = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_meter, acc_meter = AverageMeter(), AverageMeter()\n",
        "        for img, labels in val_loader:\n",
        "            # move all img, labels to device (cuda)\n",
        "            img = img.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs, _ = model(img)\n",
        "            loss_meter.update(criterion(outputs, labels).item(), len(img))\n",
        "            acc = (outputs.argmax(-1) == labels).float().mean().item()\n",
        "            acc_meter.update(acc, len(img))\n",
        "    model.train(is_train)\n",
        "    return loss_meter.calculate(), acc_meter.calculate()"
      ],
      "id": "147e1bc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69183ce"
      },
      "outputs": [],
      "source": [
        "# Time Estimate: less than 5 minutes on T4 GPU\n",
        "# train the model\n",
        "import tqdm\n",
        "for epoch in range(NUM_EPOCHS):  #\n",
        "    loss_meter = AverageMeter()\n",
        "    acc_meter = AverageMeter()\n",
        "    for img, labels in tqdm.tqdm(train_dataloader):\n",
        "        img, labels = img.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, _ = model(img)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_meter.update(loss.item(), len(img))\n",
        "        acc = (outputs.argmax(-1) == labels).float().mean().item()\n",
        "        acc_meter.update(acc, len(img))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "    print(f\"Train Epoch: {epoch}, Loss: {loss_meter.calculate()}, Acc: {acc_meter.calculate()}\")\n",
        "    if epoch % 10 == 0:\n",
        "        val_loss, val_acc = evaluate_cifar_model(model, criterion, val_dataloader)\n",
        "        print(f\"Val Epoch: {epoch}, Loss: {val_loss}, Acc: {val_acc}\")\n",
        "\n",
        "val_loss, val_acc = evaluate_cifar_model(model, criterion, val_dataloader)\n",
        "print(f\"Val Epoch: {epoch}, Loss: {val_loss}, Acc: {val_acc}\")\n",
        "print('Finished Training')"
      ],
      "id": "d69183ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22a64f80"
      },
      "source": [
        "## Part 4.D"
      ],
      "id": "22a64f80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d90ecba6"
      },
      "outputs": [],
      "source": [
        "for val_batch in val_dataloader:\n",
        "    break\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img, labels = val_batch\n",
        "    img = img.cuda()\n",
        "    outputs, attns = model(img, return_attn=True)\n",
        "\n",
        "fig, ax = plt.subplots(2, 10, figsize=(10, 2))\n",
        "for i in range(10):\n",
        "    flattened_attns = attns.flatten(1,2)[:, :, 0, 1:].mean(1).reshape(-1, 8, 8).cpu().numpy()\n",
        "    ax[0, i].imshow(inv_transform(img[i]))\n",
        "    ax[1, i].imshow(flattened_attns[i])\n",
        "    ax[0, i].axis(False)\n",
        "    ax[1, i].axis(False)"
      ],
      "id": "d90ecba6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}